\newcommand{\COURSE}{Seminar: OPT/NAS 20/21}
\newcommand{\STUDENT}{Stefan Wezel}

\documentclass[a4paper]{scrartcl}

%\newcommand{\setlabel}[1]{\edef\@currentlabel{#1}\label}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{listings}
\usepackage{tikz}
\usepackage{pdflscape}
\usepackage{subfigure}
\usepackage{float}
\usepackage{polynom}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{forloop}
\usepackage{geometry}
\usepackage{listings}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{algpseudocode,algorithm,algorithmicx}

%Definiere Let-Command für algorithmen
\newcommand*\Let[2]{\State #1 $\gets$ #2}

\input kvmacros

%Größe der Ränder setzen
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}


%\bibliographystyle{apalike}	% lengthly
%Kopf- und Fußzeile
\pagestyle {fancy}
\fancyhead[L]{\STUDENT}
\fancyhead[C]{\COURSE}
\fancyhead[R]{\today}

\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Page \thepage /\pageref*{LastPage}}
\title{\textbf{Loss Landscape Visualization}\\\small Report for the Seminar on Optimization and Neural Architecture Search}
\author{Stefan Wezel}


\begin{document}	
	
	
	% ----------------------- TODO ---------------------------
	% Hier werden die Aufgaben/Lösungen eingetragen:

%\author{Stefan Wezel}	
\maketitle
%\tableofcontents



\section*{Abstract}
\textit{Neural networks have emerged as powerful function approximators with large parameter sets. These parameters are optimized according to a loss function. Many assumptions have been made about the shape of the resulting loss landscape. However, only recently qualitative and empricial studies have been conducted. Here, we give an overview for recent advances of this field. We will explore different methods in detail and discuss their results and impact.}



\section*{Introduction}
When creating neural networks for a given task, practicioners or have to take many decisions. From architecture design, over optimizers, and schedules to hyperparameter choice there are many options that will play a key role in the eventual performance of the model. Anecdotal knowledge, experience and luck often lead to the final configuration and there is little empirical knowledge of what is actually effective. Loss landscape visualization can play a large role in guiding the community towards a more empirically foundation and maybe help build the groundwork for theoretical approaches.\\
Several recent works have shown that a better knowledge about the loss landscape can help build methods grounded in theory \cite{mutschler2020parabolic, chaudhari2019entropy}. Moreover, loss landscape visualization also helped to explain the effectiveness of existing methods such as stochastic gradient descent (SGD) \cite{robbins1951stochastic, xing2018walk} and architectures with residual connections \cite{he2016deep, li2017visualizing}.\\
As the space of parameters is too large to visualize in any meaningful way, methods of loss landscape visualization have to compromise. They might just consider small, visualizable subspace of a model's parameters. Results, thus, should be considered with caution and any conclusions should be drawn only carefully.\\
The following section will give .. to deep neural network architectures, and explain what a loss landscape is. Then, we will explore different methods of visualizing loss landscapes and see how insights from those methods can be applied. Finally, we discuss the impact of knowing the loss landscape better, see some limitations, and give an outlook on the future of the field.



\section*{Background}
In this section we will provide some theoretical background and set the denotations for the following sections.\\
A neural network $g$ is a function with a set of adjustable parameters $\theta_i$. We will denote it by $g_{\theta_i}$. Deep neural networks (DNN) can be viewed as the composition $f_{\theta} = \sigma (g_{\theta_0}) \circ ... \circ \sigma(g_{\theta_m})$ of such functions where $\sigma$ denotes some non-linear function. The parameter set $\theta$ may be arbitrary. For a given task it can be optimized by finding a minimum of a loss function $\mathcal{L}$. We refer to the resulting set as $\theta^*$. Du to the non-linear functions involved and the often large magnitude of $\theta$ a solution usually cannot be found analytically.\\
In recent years, however, iterative, algorithmic approaches have proven to reliably find sufficient minima. These approaches built on the notion of a gradient descient which dates back to \citet{cauchy1847methode}. A gradient according to the current loss $L$ and parameters $\theta_t$ is computed and a step, scaled by constant $\eta$ is taken in its negative direction. The result of a step taken is an adjusted set of parameters $\theta_{t+1}$. Note that for the gradient to be tractable, all operations involved have to be differentiable. Popular variants of the basic gradient descent algorithms include Stochastic Gradient Descent (SGD) \cite{robbins1951stochastic}, ADAM \cite{kingma2014adam}, and RMSProp \cite{graves2013generating}.\\
We refer to the surface, such an algorithm is moving on, as loss landscape. The dimension $n$ of the surface is determined by the magnitude of $\theta$. For $n=2$ this surface is easy to visualize as shown in figure~\ref{fig:twoweights}.


\begin{figure}
	\centering
	\includegraphics[width=.5\linewidth]{figures/two_weight_loss.png}
	\caption{Loss landscape for a neural network with $\theta = \lbrace w_1, w_2 \rbrace$. Values for $w_1, w_2$ are plotted against a loss $\mathcal{L}(\theta)$. As $|\theta| = 2$, visualization is straight forward.}
	\label{fig:twoweights}
\end{figure}


It is often assumed that the path such an algorithm has to take in order to find a minimum encounters several obstacles and is highly non-convex. Recent insights from loss landscape visualization have, however, shown that this is highly dependent on the architecture. \citet{li2017visualizing} have shown that loss surfaces of architectures with residual connections can have roughly convex subspaces.\\
Another common assumption is that flat minima generalize better. The intuition behind this is that there might be a shift between train and test distribution. Thus, for a flatter minimum, the probability of also covering the test distribution would be higher. However this claim was lacking empirical or theoretical studies. Recently, loss landscape visualization has helped providing compelling evidence to support such claims.



\section*{Inspecting the Loss Landscape}
With the context set and some background, we can now have a look at approaches to visualize the high-dimensional loss landscape. The following section will explore the rather simple technique of linear interpolation as proposed by \citet{goodfellow2014qualitatively}. A refined approach is taken by \citet{li2017visualizing} which will be described in the subsequent section.



\subsection*{Linear Interpolation}
%\label{subs:linear}
\citet{goodfellow2014qualitatively} propose to use a linear interpolation between two parameter sets $\theta_0$ and $\theta_1$ as visualizeable subspace. They plot the function shown in equation~\ref{eq:linear} where $\mathcal{L}$ is the loss function considering all training examples, and $\alpha$ serves as weighting parameter. The scalar $\alpha$ can for example be in a range of $0$ to $1$, thus ranging from full weight on model $\theta_0$ to interpolating towards full weigh on model $\theta_1$.

\begin{align}
\label{eq:linear}
	f(\alpha) &= \mathcal{L}((1-\alpha) \theta_0 + \alpha \theta_1)
\end{align}
The aussagekraft of such visualizations depends of course heavily on the choice of $\theta_0$ and $\theta_1$. figure~\ref{fig:untrained_trained} shows shows the interpolation between an untrained model $\theta_{untrained}$ and a model $\theta_{trained}$ trained for an image classification task.



\begin{figure}[H]
	\centering
	\includegraphics[width=.5\linewidth]{figures/linear_interpol_1.png}
	\caption{Linear interpolation between an untrained and a trained model.}
	\label{fig:untrained_trained}
\end{figure}
The subspace visualized in this figure smoothly transitions between the two models and appears roughly convex.\\
\citet{li2017visualizing} use this technique to explore the effect of batch size in training on the shape of minima. A result is shown in figure~\ref{fig:smallbatch_largebatch}. When $\alpha =0$, meaning full weight is on the model trained with a small batch size the minimum is flat and wide. As $\alpha$ increases, the loss gets higher before it sharply drops into another minima, where full weight is on the model trained with a large batch size. This minimum is much sharper, indicating a relationship between batch size and flatness of an optimum. If the assumption that flat minima generalize better holds, this would render smaller batch sizes more favorable.

\begin{figure}[H]
	\centering
	\includegraphics[width=.5\linewidth]{figures/batch_size_1.png}
	\caption{Linear interpolation between a model trained with a small batch size (left) and a model trained with a large batch size (right).}
	\label{fig:smallbatch_largebatch}
\end{figure}

As discussed by \citet{li2017visualizing}, linear interpolation suffers from several shortcomings  and limitations. For example, it is arguable whether it is actually possible to capture non-convexities with this method. They thus propose to use filter normalization when visualizing loss landscapes.



%\label{subs:filter}
\subsection*{Filter Normalization}
Addressing the issues of \citet{goodfellow2014qualitatively}'s visualizations using plain linear interpolation, \citet{li2017visualizing} propose to scale normalized parameters $\theta$. For their method, rather than interpolating between two models, inspect the neighborhood around a model $\hat{\theta}$ by adding a direction vector $u$  of the same shape to $\hat{\theta}$. This direction vector is then scaled by different values for $\alpha$ and the corresponding loss is evaluated, yielding the equation
\begin{align*}
	f(\alpha) = \mathcal{L}(\hat{\theta} + \alpha u).
\end{align*}
To ensure that the updates along $u$ live on the same scale as the the values of $\hat{\theta}$, \citet{li2017visualizing} propose to normalize $u$ by setting
\begin{align}
	u = \frac{||\hat{\theta}||}{||u||},
\end{align}

thus normalizing the parameters of the neighborhood models. They propose the term filter normalization as they conduct most of their experiments on convolutional neural networks, where the most of $\hat{\theta}$ is use to parameterize filters.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=.7\linewidth]{figures/batch_size_normalized.png}
	\caption{Loss landscape neighborhood around a model trained with a small batch size(left) and a model trained with a larger batch size.}
	\label{fig:smallbatch_largebatch_normalized}
\end{figure}
Related to figure~\ref{fig:smallbatch_largebatch}, they visualize the filter normalized landscape around a model trained with a small batch size and a model trained with a large batch size. The result is shown in figure~\ref{fig:smallbatch_largebatch_normalized}. While a difference in flatness remains, it is much less clear with both models laying in rather flat minima. The minimum of the small batch size model is, however, still noticeably flatter. The resulting test error is also lower. This further hints towards a better generalization of flat minima and the batch size's role in finding such.\\
\cite{li2017visualizing} further expand this approach beyond 1-dimensional plots. By choosing not one but multiple direction vectors $u_i$, it can be used for higher dimensions. However, any dimension beyond two would result in the same dilemma for which such methods were developed. Thus, applying it to two dimensions is useful. The function to plot is then
\begin{align*}
		f(\alpha, \beta) = \mathcal{L}(\hat{\theta} + \alpha u_1 + \beta u_2).
\end{align*}
This results in more expressive plots, revealing more information about the $\hat{\theta}$'s neighborhood. An insightful example is given in figure~\ref{fig:2dloss} where two architectural paradigms are compared. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/2dloss.pdf}
	\caption{Loss landscape neighborhood around a model without residual connections (left) and a model with residual connections.}
	\label{fig:2dloss}
\end{figure}
The loss landscape around a minimum found with a model without any residual connections has several local minima. Intuitively put, an algorithm such as SGD might have a hard time traversing such rugged terrain. On the other hand, introducing residual connection results in a much smoother loss landscape around the minimum. It even appears roughly convex, in the visualized subspace.\\
While the methods proposed by \cite{li2017visualizing} produce more nuanced plots than simple linear interpolation, it is still important to mention that only a small subspace of the high dimensional parameter space is depicted. Thus, any conclusion has to be drawn with caution. It is also worth mentioning that their proposed method comes with high computational costs, as they consider the whole dataset to evaluate the empirical loss $\mathcal{L}$ at any considered point around $\hat{\theta}$. This 
severely limits the applicability of such methods. For reasonable experiments, only as small patch of neighborhood can be considered. Also the variety of currently existing architectures expands beyond just using residual connections or not. Moreover, this makes it also only usable with constraints for exploring the effect of different values for hyperparameters on the loss landscape.









\section*{Applying Findings from Loss Landscape Visualization}
Recent progress in loss landscape visualization, as the works described in the previous section, have brought new insights on the shape of the loss function. These insight have also helped build a better understanding of the impact on loss's shape on generalization. In the following sections, we will focus on works that apply these insight to build optimizers.


\subsection*{Parabolic Approximation Line Search}
We have seen that, depending on architecture, the loss surface can be roughly convex. \cite{mutschler2020parabolic} provide further empirical evidence to support this assumption. To make use of this characteristic, they propose an optimizer that approximates the loss in negative gradient with a parabola. A step is then taken to the minimum of the parabola, which can be found analytically. They call their approach Parabolic Approximation Line Search (PAL).

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\linewidth]{figures/pal_1.png}
	\caption{Intuition for PAL. Three measurements are taken and a parabola is approximated according to them. The update step is then taken to the minimum of the parabola.}
	\label{fig:pal}
\end{figure}
To approximate a parabola, PAL takes three measurements. The loss $L(0)$ at the current position, the derivative in gradient direction $L'(0) = - ||g_t||$, and finally the a loss at another position $L(\mu)$, where the measuring distance $\mu$ is a positive scalar. These measurements are now used to parameterize parabola $\hat{L}(s) = as^2 + bs + c$. The curvature is determined by $a = \frac{L(\mu)-L(0) - L'(0)\mu}{\mu^2}$. The position of the parabola is given by $b=L'(0)$ and $c=L(0)$.\\
Given this approximated parabola, PAL performs an update step $s_{update}$ to its minimum by
\begin{align}
	s_{update} = \frac{\hat{L}'(0)}{\hat{L}''(0)} = -\frac{b}{2a} = \frac{-\hat{L}'(0)}{2\frac{L(\mu)-L(0) - L'(0)\mu}{\mu^2}}.
\end{align}
For PAL to be able to perform such an update step, $L(0)$ and $L(\mu)$ have to be determined. For the very first step, this cannot be achieved by PAL itself. To alleviate this, \cite{mutschler2020parabolic} propose to perform a cold start using another algorithm to find  $L(\mu)$. It is also important to note that the measurements have to be taken on the same batch and all random values involved have to be reused.\\
\cite{mutschler2020parabolic} compare the performance of PAL to other optimizers. They consider commonly used approaches like SGD or ADAM as well as to other line search methods such as Stochastic Line Search (SLS) proposed by \cite{paquette2018stochastic}. Their results show that PAL's performance is consistent across multiple architectures and tasks. This becomes especially evident when compared to SLS, which while outperforming PAL on some tasks, is much less stable. PAL generally converges to good minima without relying on hand-crafted step size schedules, as for example SGD does.\\
To further justify their method, \cite{mutschler2020parabolic} also provide extensive evidence towards the loss landscape's local convexity. Despite the success of the proposed PAL's it is still worth mentioning that on a number of tasks, SGD with a learning rate schedule outperforms other optimizers including line search approaches such as PAL.



\subsection*{Entropy-SGD}
\cite{chaudhari2019entropy} build on the notion that flat minima yield better generalization. They actively bias their optimizer to find wide valleys. Extending the work of \cite{baldassi2016unreasonable}, rather than considering a landscape determined only by loss, they consider the volume of similar parameter configurations around the current model and thus form what they call entropy landscape.

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\linewidth]{figures/entropy_1.png}
	\caption{Intuition for Entropy-SGD. The entropy loss has a global minimum where generalization is assumed to be high. TODO change x to theta}
	\label{fig:entropy}
\end{figure}
An intuition for this entropy landscape is given in figure~\ref{fig:entropy}. In this toy example the landscape formed by the used loss (black) has two major minima. One, however, is quite sharp while the other is much flatter. Two more curves (red) denote the local entropy landscape as proposed by \cite{chaudhari2019entropy}. These curves global minima both lie in the original loss landscapes flatter minimum. \cite{chaudhari2019entropy} assume that using the model lying in the global minimum of the local entropy result in better generalization.\\
The introduced local entropy is given by
\begin{align}
	F(\theta, \gamma) &= log\; \int_\theta exp(-L(\theta') - \frac{\gamma}{2}||\theta-\theta'||_2^2 d\theta') 
\end{align},where $\theta$ is a model considered , $\theta'$ is a model in $\theta$'s neighborhood, and $\gamma$ can be viewed as a scope that determines, how far away, models should be considered to compute the volume. The mass of this local entropy concentrates in wide valleys and is thus maximized.\\
The proposed Entropy algorithm finds finds a minimum of the negative local entropy landscape by performing a nested loop of two SGDs. Intuitively put, the inner SGD estimates the gradient of the local entropy based on the volume of good models while the outer SGD updates parameters $\theta$. Thus Entropy-SGD computes
\begin{align*}
	\theta*_{Entropy-SGD} = arg\;min_{\theta} - F(\theta, \gamma; X)
\end{align*}
which can be found by iteratively computing 
\begin{align}
	-\nabla_{\theta}F(\theta, \gamma; X_{batch}) = \gamma (\theta - \mathbb{E}\left[ \theta'; X_{batch} \right]).
\end{align}
\citet{chaudhari2019entropy} describe the effect of $\gamma$ as follows: for $\gamma \rightarrow 0$, the entropy landscape approaches the regular loss landscape as determined by the objective. For $\gamma \rightarrow \infty$, the entropy landscape approaches a uniform distribution.\\
To support their claims and methods, \citet{chaudhari2019entropy} provide further theoretical evidence. They proof that the entropy landscape is smoother than the original loss landscape. Analyzing the eigenspectrum of the Hessian \cite{bishop1992exact}, of models that generalize well,the majority of eigenvalues is close to zero. This indicates low curvature and thus flatness of the minima, further backing the initial assumption that flatter minima generalize better.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=.5\linewidth]{figures/eigenspectrum.png}
	\caption{Histogram of the eigenspectrum of the Hessian matrix of a well generalizing model.}
	\label{fig:eigenspectrum}
\end{figure}
Empirically, \citet{chaudhari2019entropy} compare Entropy-SGD to SGD and ADAM. In their experiments, they demonstrate that on the tasks considered, Entropy-SGD reaches lower test error/perplexity.\\
However, it is important to note that the conducted experiments only encompass a rather small set of tasks. In order to empirically proof that Entropy-SGD is able to outperform state-of-the-art opimizers further investigation is needed, and a wider set of tasks has to be considered. Nonetheless, this work bring yet more insights into the nature of the loss landscape and more specifically generalization.


\section*{Discussion or Conclusion}
%The methods presented are interesting examples investigating  that are often assumed to hold.


\section{Conclusions}
\label{sec:conc}
Modern deep neural networks are powerful and are increasingly applied for challenging tasks. While this field of research is quickly evolving, the development is often ahead of itself. This is exemplified by the seemingly unreasonable performance of algorithms like SGD. Only by rigorous investigating methods, finding empirical and theoretical evidence, the field can progress sustainably.\\
The works presented here are displays of this process. While not necessarily providing groundbreaking insights to the loss landscape's shape or yielding a new do-it-all optimizer, they are important steps towards building a solid foundation for optimziation deep learning.



\newpage
\bibliographystyle{alpha}
\bibliography{references}

	
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: